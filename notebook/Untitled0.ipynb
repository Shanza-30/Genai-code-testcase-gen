{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMZhE3ehwy88"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('software_requirements_extended.csv')"
      ],
      "metadata": {
        "id": "33q1DkC2F2kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "r87e-dcpGDP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "hsbhUTPjGF1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "NatvTdLVGKaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "MkvjqVcGI5CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling missing values**"
      ],
      "metadata": {
        "id": "jGJT_fWBdfFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "sPFZWlCHI9EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum() / len(df) * 100"
      ],
      "metadata": {
        "id": "rqHK6ZLCJF_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Duplicates\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "pX8lK9b6JcvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate requirements\n",
        "df = df.drop_duplicates(subset=['Requirement'])\n"
      ],
      "metadata": {
        "id": "3_H_urB8Jl9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Robl8-cBJp_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "N2FHt5M9JwXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "id": "3A8MxRKiZPB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cleaning**"
      ],
      "metadata": {
        "id": "idBxAYB7aFk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercasing\n",
        "df['clean_text'] = df['Requirement'].str.lower()\n",
        "print(\"After Lowercasing\")\n",
        "print(df[['Requirement','clean_text']], \"\\n\")"
      ],
      "metadata": {
        "id": "o7r3PUPxVC4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove Punctuation, Numbers & Special Characters\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "print(\"After Removing Punctuation & Numbers\")\n",
        "print(df[['Requirement','clean_text']], \"\\n\")"
      ],
      "metadata": {
        "id": "KXloEjFyVLyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "df['tokens'] = df['clean_text'].apply(lambda x: word_tokenize(x))\n",
        "print(\"After Tokenization\")\n",
        "print(df[['Requirement','tokens']], \"\\n\")"
      ],
      "metadata": {
        "id": "2xkIN_UAVNgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "print(\"After Stopword Removal\")\n",
        "print(df[['Requirement','tokens']], \"\\n\")\n"
      ],
      "metadata": {
        "id": "ITHmKqUMVV2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "print(\"After Lemmatization\")\n",
        "print(df[['Requirement','tokens']], \"\\n\")\n"
      ],
      "metadata": {
        "id": "NuQABsf9Va8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Join Back Tokens\n",
        "df['clean_text'] = df['tokens'].apply(lambda x: \" \".join(x))\n",
        "print(\"Final Cleaned Text\")\n",
        "print(df[['Requirement','clean_text']], \"\\n\")"
      ],
      "metadata": {
        "id": "jDxLXzOdYxoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "KFWp7DP1YziI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Type'].value_counts()"
      ],
      "metadata": {
        "id": "FooOQ9WAc1lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"clean_text\"] = df[\"Requirement\"].astype(str).str.lower()\n"
      ],
      "metadata": {
        "id": "G7ZethETN4ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'] = df['clean_text'].astype(str)\n",
        "\n",
        "# Map 'Type' to binary label: 1 for Functional (FR, F), 0 for Non-functional (all others)\n",
        "functional_types = ['FR', 'F']\n",
        "df['label'] = df['Type'].apply(lambda x: 1 if x in functional_types else 0)\n",
        "\n",
        "print(df[['Type', 'label', 'clean_text']].head())"
      ],
      "metadata": {
        "id": "L9XcQskyG4JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Suppose your class column is named \"Requirement\"\n",
        "# le = LabelEncoder()\n",
        "# df[\"label\"] = le.fit_transform(df[\"Requirement\"]) # Remove this line as it overwrites the binary label"
      ],
      "metadata": {
        "id": "C2ePEdMNpQJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"clean_text\"] = df[\"Requirement\"].astype(str).str.lower()\n"
      ],
      "metadata": {
        "id": "lwQJuw_4po_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=[\"clean_text\", \"Requirement\", \"label\"])\n"
      ],
      "metadata": {
        "id": "H2H1-K3-pzjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seaborn\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=\"label\", data=df, palette=\"coolwarm\")  # Use 'label' instead of 'target'\n",
        "plt.title(\"Class Distribution (Functional=1, Non-Functional=0)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ycy-LY_mHxlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Class distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "df[\"label\"].value_counts().plot(kind=\"bar\", color=\"skyblue\")\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.xlabel(\"Labels\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "InE4OdtW5w58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Count Plot**"
      ],
      "metadata": {
        "id": "C-JSEKddd5Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution of Requirement Types (FR vs NFR)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='Type', data=df, palette=\"Set2\")\n",
        "plt.title(\"Distribution of Requirement Types (FR vs NFR)\")\n",
        "plt.xlabel(\"Requirement Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M_xntgn3cvtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Histogram**"
      ],
      "metadata": {
        "id": "cwgya2PAeGI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pick column: prefer 'clean_text', otherwise fall back to 'Requirement'\n",
        "col = 'clean_text' if 'clean_text' in df.columns else 'Requirement'\n",
        "\n",
        "# compute length safely (handles NaN)\n",
        "df['text_length'] = df[col].astype(str).str.split().str.len()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(df['text_length'], bins=30, kde=True, color=\"purple\")\n",
        "plt.title(f\"Sentence Length Distribution ({col})\")\n",
        "plt.xlabel(\"Number of Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oc5mzoI5tJgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Most Frequent Words\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Combine all requirements into one text\n",
        "all_words = ' '.join(df['Requirement'].astype(str)).lower().split()\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in all_words if word not in stop_words]\n",
        "\n",
        "# Get most common words\n",
        "common_words = Counter(filtered_words).most_common(15)\n"
      ],
      "metadata": {
        "id": "_YpGfJV3dJKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar plot\n",
        "words, counts = zip(*common_words)\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=list(counts), y=list(words), palette=\"mako\")\n",
        "plt.title(\"Top 15 Most Frequent Words in Requirements\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Word\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wTq4szhDii5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WordCloud for Visualization\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(filtered_words))\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"WordCloud of Software Requirements\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N196EzJtdTRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for Functional\n",
        "func_text = \" \".join(df[df[\"label\"]==1][\"clean_text\"])\n",
        "non_func_text = \" \".join(df[df[\"label\"]==0][\"clean_text\"])\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "wc = WordCloud(width=800, height=400, background_color=\"white\").generate(func_text)\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"WordCloud - Functional Requirements\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QwscaW2UrhQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud for Non_Functional\n",
        "plt.figure(figsize=(10,5))\n",
        "wc = WordCloud(width=800, height=400, background_color=\"white\").generate(non_func_text)\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"WordCloud - Non-Functional Requirements\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EX2_725drkFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate IQR\n",
        "Q1 = df['text_length'].quantile(0.25)\n",
        "Q3 = df['text_length'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Outlier condition\n",
        "outliers = (df['text_length'] < (Q1 - 1.5 * IQR)) | (df['text_length'] > (Q3 + 1.5 * IQR))\n",
        "\n",
        "# Remove outliers\n",
        "df_no_outliers = df[~outliers].copy()\n",
        "\n",
        "print(\"Original Data shape:\", df.shape)\n",
        "print(\"Data without Outliers shape:\", df_no_outliers.shape)"
      ],
      "metadata": {
        "id": "Q--eyr1dflCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Apply Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "df_no_outliers['text_length_normalized'] = scaler.fit_transform(df_no_outliers[['text_length']])\n",
        "\n",
        "print(df_no_outliers)"
      ],
      "metadata": {
        "id": "6cLlshDYgO6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "df_no_outliers['text_length_scaled'] = scaler.fit_transform(df_no_outliers[['text_length']])\n",
        "\n",
        "print(df_no_outliers)"
      ],
      "metadata": {
        "id": "h-e77_53gZ9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "SPGg3P-bhJOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Engineering\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=20)\n",
        "X_tfidf = vectorizer.fit_transform(df['clean_text'])\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"TF-IDF Features:\\n\", feature_names)\n",
        "print(\"\\nTF-IDF Matrix:\\n\", X_tfidf.toarray())"
      ],
      "metadata": {
        "id": "4P9UVdqOhSr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra features: word count & char count\n",
        "df[\"word_count\"] = df[\"clean_text\"].apply(lambda x: len(x.split()))\n",
        "df[\"char_count\"] = df[\"clean_text\"].apply(len)\n",
        "\n",
        "print(df[[\"clean_text\", \"word_count\", \"char_count\"]].head())\n"
      ],
      "metadata": {
        "id": "brHllkTZvHrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Selection\n",
        "selector = SelectKBest(score_func=chi2, k=10)  # top 10 features\n",
        "X_selected = selector.fit_transform(X_tfidf, df['Type'])\n",
        "\n",
        "selected_features = feature_names[selector.get_support()]\n",
        "print(\"\\nSelected Features (Chi2):\", selected_features)"
      ],
      "metadata": {
        "id": "woLpyq9Phgqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_tfidf, df['Type'])\n",
        "\n",
        "importances = model.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nRandomForest Feature Importances:\\n\", importance_df)"
      ],
      "metadata": {
        "id": "Bnbl9ESnhpe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features + Labels\n",
        "X = df[\"clean_text\"]\n",
        "y = df[\"Type\"]"
      ],
      "metadata": {
        "id": "fYw0-68YnBAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Bag of Words\n",
        "bow = CountVectorizer(stop_words=\"english\")\n",
        "X_bow = bow.fit_transform(df['clean_text'])  # Sparse matrix\n",
        "\n",
        "# Convert sparse matrix to dense array and then to DataFrame (Optional, for inspection)\n",
        "# X_bow_df = pd.DataFrame(X_bow.toarray(), columns=bow.get_feature_names_out())\n",
        "# print(X_bow_df.head())\n",
        "\n",
        "# Split data into training and testing sets for BOW features\n",
        "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(X_bow, df['Type'], test_size=0.3, random_state=42)\n",
        "\n",
        "print(\"Bag of Words features created and data split.\")"
      ],
      "metadata": {
        "id": "jDI4lSl4ZMb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, df[\"label\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n"
      ],
      "metadata": {
        "id": "It3YwhrItm6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Improved TF-IDF with more features, trigrams, and sublinear scaling\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=30000,\n",
        "    ngram_range=(1, 3),\n",
        "    stop_words='english',\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "# âœ… Convert to DataFrame for inspection\n",
        "X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "print(X_tfidf_df.head())\n"
      ],
      "metadata": {
        "id": "0GG8qBEIELub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Drop NaN values from both X and y together\n",
        "df = df.dropna(subset=[\"clean_text\", \"Type\"])\n",
        "\n",
        "# Step 2: Features aur labels banaye\n",
        "X = df[\"clean_text\"]\n",
        "y_binary = df[\"Type\"].apply(lambda x: 1 if x in functional_types else 0).values\n",
        "\n",
        "print(\"Length of X:\", len(X))\n",
        "print(\"Length of y_binary:\", len(y_binary))\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n",
        ")\n"
      ],
      "metadata": {
        "id": "SB91OBy3YQng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "xDW_3Rbbf-sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "MeqEVMmHgeOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "id": "TmS25Kl6ghSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "HSMmLTwcglKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model**"
      ],
      "metadata": {
        "id": "vbtZahhavcNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Convert text into TF-IDF features\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))  # unigrams + bigrams\n",
        "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, df[\"label\"], test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# 3. Train Logistic Regression model\n",
        "lr_model = LogisticRegression(C=10, penalty=\"l2\", solver=\"lbfgs\", max_iter=2000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# 5. Evaluation Metrics (Stored in Variables)\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "prec_lr = precision_score(y_test, y_pred_lr, average='weighted')\n",
        "rec_lr = recall_score(y_test, y_pred_lr, average='weighted')\n",
        "f1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n",
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "\n",
        "# âœ… Duplicate variables for comparison table\n",
        "y_pred_log = y_pred_lr\n",
        "acc_log = acc_lr\n",
        "prec_log = prec_lr\n",
        "rec_log = rec_lr\n",
        "f1_log = f1_lr\n",
        "\n",
        "# 6. Display Results\n",
        "print(\"Logistic Regression (Optimized) Performance:\")\n",
        "print(f\" Accuracy  : {acc_lr:.4f}\")\n",
        "print(f\" Precision : {prec_lr:.4f}\")\n",
        "print(f\" Recall    : {rec_lr:.4f}\")\n",
        "print(f\"F1-Score  : {f1_lr:.4f}\")\n",
        "print(\" Confusion Matrix:\\n\", cm_lr)\n"
      ],
      "metadata": {
        "id": "AkCXTBVkk-L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cross-Validation (Example with Logistic Regression)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "log_reg_cv = LogisticRegression(max_iter=1000)\n",
        "scores = cross_val_score(log_reg_cv, X_tfidf, df[\"label\"], cv=5, scoring=\"accuracy\")\n",
        "\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Mean accuracy:\", np.mean(scores))\n"
      ],
      "metadata": {
        "id": "0L0HAMGG54Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert text into TF-IDF features (with unigrams + bigrams)\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, df[\"label\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Naive Bayes with tuned alpha\n",
        "nb_model = MultinomialNB(alpha=0.3)  # try 0.1, 0.3, 0.5\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nNaive Bayes Performance:\")\n",
        "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
        "prec_nb = precision_score(y_test, y_pred_nb, average='weighted')\n",
        "rec_nb = recall_score(y_test, y_pred_nb, average='weighted')\n",
        "f1_nb = f1_score(y_test, y_pred_nb, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", round(acc_nb, 4))\n",
        "print(\"Precision:\", round(prec_nb, 4))\n",
        "print(\"Recall:\", round(rec_nb, 4))\n",
        "print(\"F1 Score:\", round(f1_nb, 4))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nb))\n"
      ],
      "metadata": {
        "id": "CLxjhuaClMBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert text to TF-IDF\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, df[\"label\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest model (stronger settings but no hyperparameter grid)\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=300,     # more trees â†’ better accuracy\n",
        "    max_depth=40,         # deeper trees â†’ capture patterns\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nRandom Forest Performance :\")\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "prec_rf = precision_score(y_test, y_pred_rf, average='weighted')\n",
        "rec_rf = recall_score(y_test, y_pred_rf, average='weighted')\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", round(acc_rf, 4))\n",
        "print(\"Precision:\", round(prec_rf, 4))\n",
        "print(\"Recall:\", round(rec_rf, 4))\n",
        "print(\"F1 Score:\", round(f1_rf, 4))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "5xTztVi4lh6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Convert text to TF-IDF vectors\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, df[\"label\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 200],\n",
        "    \"max_depth\": [10, 20, None],\n",
        "    \"min_samples_split\": [2, 5]\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(), param_grid, cv=3,\n",
        "                       scoring=\"accuracy\", n_jobs=-1)\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_rf.best_params_)\n",
        "print(\"Best Accuracy:\", grid_rf.best_score_)\n"
      ],
      "metadata": {
        "id": "QmiXt18t5-O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert text to TF-IDF with unigrams + bigrams\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, df[\"label\"], test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# Model\n",
        "svm_model = LinearSVC(random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nSVM (LinearSVC with TF-IDF) Performance:\")\n",
        "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
        "prec_svm = precision_score(y_test, y_pred_svm, average='weighted')\n",
        "rec_svm = recall_score(y_test, y_pred_svm, average='weighted')\n",
        "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", round(acc_svm, 4))\n",
        "print(\"Precision:\", round(prec_svm, 4))\n",
        "print(\"Recall:\", round(rec_svm, 4))\n",
        "print(\"F1 Score:\", round(f1_svm, 4))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "id": "8R7XXbVklp9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert text to TF-IDF with unigrams + bigrams\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(df[\"label\"])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y_enc, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# Model (simple setup, no heavy tuning)\n",
        "xgb_model = XGBClassifier(eval_metric=\"mlogloss\", use_label_encoder=False, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nXGBoost Performance with TF-IDF:\")\n",
        "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "prec_xgb = precision_score(y_test, y_pred_xgb, average='weighted')\n",
        "rec_xgb = recall_score(y_test, y_pred_xgb, average='weighted')\n",
        "f1_xgb = f1_score(y_test, y_pred_xgb, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", round(acc_xgb, 4))\n",
        "print(\"Precision:\", round(prec_xgb, 4))\n",
        "print(\"Recall:\", round(rec_xgb, 4))\n",
        "print(\"F1 Score:\", round(f1_xgb, 4))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "id": "kUODy7L8mZs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Comparison Table\n",
        "import pandas as pd\n",
        "\n",
        "results = {\n",
        "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"Naive Bayes\", \"SVM\", \"XGBoost\"],\n",
        "    \"Accuracy\": [acc_log, acc_rf, acc_nb, acc_svm, acc_xgb],\n",
        "    \"Precision\": [prec_log, prec_rf, prec_nb, prec_svm, prec_xgb],\n",
        "    \"Recall\": [rec_log, rec_rf, rec_nb, rec_svm, rec_xgb],\n",
        "    \"F1-Score\": [f1_log, f1_rf, f1_nb, f1_svm, f1_xgb]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "br_vpyTC6V5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DL Model**"
      ],
      "metadata": {
        "id": "UsNvAwUfyvIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical # Import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Tokenization & Padding\n",
        "max_words = 2000       # Vocabulary size\n",
        "max_len = 50           # Max sequence length\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df['clean_text'])\n",
        "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Use the binary 'label' column (0 or 1) for the target variable\n",
        "y = df['label'].values # This column already contains binary labels (0 or 1)\n",
        "\n",
        "# No need to one-hot encode for sparse_categorical_crossentropy with integer labels\n",
        "# If using binary_crossentropy and sigmoid output, keep as 1D array (0 or 1)"
      ],
      "metadata": {
        "id": "vQ-5LJ7ppDVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Library\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Use the binary 'label' column for splitting\n",
        "y = df['label'].values # Use the binary label column for splitting\n",
        "\n",
        "# Ensure the stratify variable is a clean 1D binary array (0 or 1)\n",
        "# Filter out any potential non-binary values if they exist\n",
        "binary_labels = y\n",
        "# If your 'label' column might contain values other than 0 and 1,\n",
        "# you might need more robust filtering here. Assuming it only contains 0s and 1s\n",
        "# or can be safely cast to int.\n",
        "stratify_labels = binary_labels.astype(int)\n",
        "\n",
        "# Check class counts before splitting\n",
        "unique_labels, counts = np.unique(stratify_labels, return_counts=True)\n",
        "print(\"Class counts for stratification:\", dict(zip(unique_labels, counts)))\n",
        "\n",
        "# Split - Use stratified split\n",
        "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(\"X_train_lstm shape:\", X_train_lstm.shape)\n",
        "print(\"y_train_lstm shape:\", y_train_lstm.shape)"
      ],
      "metadata": {
        "id": "z5zQc1eRpHpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dense(32, activation='relu'))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "lstm_model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "5OD1nW23pKvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Encode labels first\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df[\"label\"])\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df[\"clean_text\"])\n",
        "X_seq = pad_sequences(sequences, maxlen=max_len)"
      ],
      "metadata": {
        "id": "LabxTQEOqtlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(\n",
        "    X_seq, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train_lstm shape:\", X_train_lstm.shape)\n",
        "print(\"y_train_lstm shape:\", y_train_lstm.shape)"
      ],
      "metadata": {
        "id": "DbJ23ISrreJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dense(32, activation=\"relu\"))\n",
        "lstm_model.add(Dense(len(le.classes_), activation=\"softmax\"))\n",
        "\n",
        "lstm_model.compile(optimizer=Adam(0.001),\n",
        "                   loss=\"sparse_categorical_crossentropy\",\n",
        "                   metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "QTWB8CPGrSZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Train model\n",
        "history = lstm_model.fit(\n",
        "    X_train_lstm, y_train_lstm,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "w83S5oAfrXQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_lstm_proba = lstm_model.predict(X_test_lstm)\n",
        "\n",
        "# Convert probabilities to labels\n",
        "if y_pred_lstm_proba.shape[-1] > 1:   # softmax (multi-class / one-hot)\n",
        "    y_pred_lstm = np.argmax(y_pred_lstm_proba, axis=1)\n",
        "else:  # sigmoid (binary)\n",
        "    y_pred_lstm = (y_pred_lstm_proba > 0.5).astype(int)\n",
        "\n",
        "# Convert y_test if one-hot encoded\n",
        "if len(y_test_lstm.shape) > 1 and y_test_lstm.shape[-1] > 1:\n",
        "    y_test_lstm_binary = np.argmax(y_test_lstm, axis=1)\n",
        "else:\n",
        "    y_test_lstm_binary = y_test_lstm\n",
        "\n",
        "# ðŸ“Š Evaluation\n",
        "acc = accuracy_score(y_test_lstm_binary, y_pred_lstm)\n",
        "prec = precision_score(y_test_lstm_binary, y_pred_lstm, average='weighted', zero_division=0)\n",
        "rec = recall_score(y_test_lstm_binary, y_pred_lstm, average='weighted')\n",
        "f1 = f1_score(y_test_lstm_binary, y_pred_lstm, average='weighted')\n",
        "cm = confusion_matrix(y_test_lstm_binary, y_pred_lstm)\n",
        "\n",
        "print(\"\\nLSTM Model Performance:\")\n",
        "print(\"Accuracy:\", round(acc, 4))\n",
        "print(\"Precision:\", round(prec, 4))\n",
        "print(\"Recall:\", round(rec, 4))\n",
        "print(\"F1 Score:\", round(f1, 4))\n",
        "print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "0_VTofxepT1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# âœ… Step 1: Prepare data\n",
        "texts = df[\"clean_text\"].astype(str).tolist()\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(df[\"label\"])\n",
        "\n",
        "# Tokenize\n",
        "max_words = 10000\n",
        "max_len = 100  # you can tune this\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, labels, test_size=0.2, random_state=0, stratify=labels\n",
        ")\n",
        "\n",
        "# âœ… Step 2: Build BiLSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dense(len(le.classes_), activation=\"softmax\"))  # multi-class\n",
        "\n",
        "# Compile\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# âœ… Step 3: Train\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,          # you can try 15â€“20 for better accuracy\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# âœ… Step 4: Evaluate\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nBiLSTM Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "dBxvXR6tN7iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# âœ… Predict\n",
        "y_pred_probs = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# âœ… Evaluation\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred, average='weighted')\n",
        "rec = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\nBiLSTM Performance:\")\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall:    {rec:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "_GyI5jKpOm6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=2)"
      ],
      "metadata": {
        "id": "eyF9FLkMwtGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "# CNN Model\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "cnn_model.add(Conv1D(128, 5, activation=\"relu\"))       # Convolutional layer\n",
        "cnn_model.add(GlobalMaxPooling1D())                    # Pooling layer\n",
        "cnn_model.add(Dense(32, activation=\"relu\"))            # Fully connected layer\n",
        "cnn_model.add(Dense(1, activation=\"sigmoid\"))          # Output layer for binary classification\n",
        "\n",
        "cnn_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "cnn_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "XPnRGoaK0CFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Step 1: Predict probabilities on test set\n",
        "y_pred_prob = cnn_model.predict(X_test)\n",
        "\n",
        "# Step 2: Convert probabilities to class labels (binary)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Step 3: Compute metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 4: Print results\n",
        "print(\"CNN Model Performance:\")\n",
        "print(\"Accuracy :\", round(accuracy, 4))\n",
        "print(\"Precision:\", round(precision, 4))\n",
        "print(\"Recall   :\", round(recall, 4))\n",
        "print(\"F1 Score :\", round(f1, 4))\n",
        "print(\"Confusion Matrix:\\n\", cm)\n"
      ],
      "metadata": {
        "id": "62WnpWcjzXky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# âœ… Store DL model results\n",
        "acc_lstm, prec_lstm, rec_lstm, f1_lstm = 0.8827, 0.8834, 0.8827, 0.8828\n",
        "acc_bilstm, prec_bilstm, rec_bilstm, f1_bilstm = 0.8622, 0.8628, 0.8622, 0.8623\n",
        "acc_cnn, prec_cnn, rec_cnn, f1_cnn = 0.8418, 0.8472, 0.8418, 0.8419\n",
        "\n",
        "# âœ… Create a comparison table (Correct Order: LSTM â†’ BiLSTM â†’ CNN)\n",
        "dl_results = {\n",
        "    \"Model\": [\"LSTM\", \"BiLSTM\", \"CNN\"],\n",
        "    \"Accuracy\": [acc_lstm, acc_bilstm, acc_cnn],\n",
        "    \"Precision\": [prec_lstm, prec_bilstm, prec_cnn],\n",
        "    \"Recall\": [rec_lstm, rec_bilstm, rec_cnn],\n",
        "    \"F1-Score\": [f1_lstm, f1_bilstm, f1_cnn]\n",
        "}\n",
        "\n",
        "dl_results_df = pd.DataFrame(dl_results)\n",
        "print(\"\\nDeep Learning Model Comparison Table:\")\n",
        "print(dl_results_df)\n"
      ],
      "metadata": {
        "id": "Sh1UAzvIRKUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay, confusion_matrix"
      ],
      "metadata": {
        "id": "aJmeimcntkWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Radar Chart (Performance Metrics: Accuracy, Precision, Recall, F1)\n",
        "def plot_radar(model_name, accuracy, precision, recall, f1):\n",
        "    labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "    stats = [accuracy, precision, recall, f1]\n",
        "\n",
        "    angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
        "    stats += stats[:1]  # close the circle\n",
        "    angles += angles[:1]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6,6), subplot_kw=dict(polar=True))\n",
        "    ax.plot(angles, stats, 'o-', linewidth=2, label=model_name)\n",
        "    ax.fill(angles, stats, alpha=0.25)\n",
        "    ax.set_thetagrids(np.degrees(angles[:-1]), labels)\n",
        "    ax.set_ylim(0,1)\n",
        "    plt.title(f'{model_name} Performance Radar Chart')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "aWiSd-P8ubZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_radar(\"Logistic Regression\", 0.85, 0.83, 0.82, 0.825)\n"
      ],
      "metadata": {
        "id": "QSKsO23zz94K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# âœ… Model names & results â€” directly use your computed variables\n",
        "models = [\"Logistic Regression\", \"Random Forest\", \"Naive Bayes\", \"SVM\", \"XGBoost\", \"LSTM\", \"BiLSTM\", \"CNN\"]\n",
        "\n",
        "accuracy  = [acc_log, acc_rf, acc_nb, acc_svm, acc_xgb, acc_lstm, acc_bilstm, acc_cnn]\n",
        "precision = [prec_log, prec_rf, prec_nb, prec_svm, prec_xgb, prec_lstm, prec_bilstm, prec_cnn]\n",
        "recall    = [rec_log, rec_rf, rec_nb, rec_svm, rec_xgb, rec_lstm, rec_bilstm, rec_cnn]\n",
        "f1        = [f1_log, f1_rf, f1_nb, f1_svm, f1_xgb, f1_lstm, f1_bilstm, f1_cnn]\n",
        "\n",
        "# âœ… Combine metrics dynamically\n",
        "metrics = [accuracy, precision, recall, f1]\n",
        "metric_labels = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "\n",
        "# âœ… Calculate angles\n",
        "angles = np.linspace(0, 2 * np.pi, len(metric_labels), endpoint=False).tolist()\n",
        "angles += angles[:1]\n",
        "\n",
        "# âœ… Radar chart\n",
        "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    # Skip models with missing results\n",
        "    if None in [accuracy[i], precision[i], recall[i], f1[i]]:\n",
        "        continue\n",
        "\n",
        "    values = [m[i] for m in metrics]\n",
        "    values += values[:1]\n",
        "    ax.plot(angles, values, label=model)\n",
        "    ax.fill(angles, values, alpha=0.1)\n",
        "\n",
        "# âœ… Set axis labels dynamically\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(metric_labels)\n",
        "\n",
        "plt.title(\"Model Performance Comparison (Auto-Updated Radar Chart)\", size=14)\n",
        "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vUaWEZ6_pUg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC Curve (Binary Classification)\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def plot_roc(y_true, y_scores, model_name):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0,1.0])\n",
        "    plt.ylim([0.0,1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve - {model_name}')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "Z2oy2clxrTXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Make sure you are using TF-IDF X_test, not the LSTM one\n",
        "# Re-run your TF-IDF transformation and train-test split before ROC curve\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(\n",
        "    X_tfidf, df[\"label\"], test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# Refit Logistic Regression if needed\n",
        "lr_model.fit(X_train_lr, y_train_lr)\n",
        "\n",
        "# âœ… Get probability scores (now dimensions match)\n",
        "y_scores_lr = lr_model.predict_proba(X_test_lr)[:, 1]\n",
        "y_true_lr_roc_binary = y_test_lr\n",
        "\n",
        "# âœ… ROC Curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true_lr_roc_binary, y_scores_lr)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Logistic Regression')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9B8zRwHBvkwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For LSTM:\n",
        "# Ensure y_test_lstm is in a single column binary format if it's one-hot encoded\n",
        "if len(y_test_lstm.shape) > 1 and y_test_lstm.shape[-1] > 1:\n",
        "    y_test_lstm_binary = np.argmax(y_test_lstm, axis=1)\n",
        "elif len(y_test_lstm.shape) == 1 and np.all(np.isin(y_test_lstm, [0, 1])):\n",
        "     y_test_lstm_binary = y_test_lstm # Already in 1D binary format\n",
        "else:\n",
        "    # If neither of the above, try to convert to binary assuming it's a multiclass integer array\n",
        "    # This might be needed if the previous steps resulted in a 1D integer array of multiple classes\n",
        "    print(\"Attempting to convert multiclass integer labels to binary (0 or 1)...\")\n",
        "    # Assuming 1 is the positive class and others are negative\n",
        "    y_test_lstm_binary = (y_test_lstm == 1).astype(int)\n",
        "    print(\"Unique values after attempted conversion:\", np.unique(y_test_lstm_binary))\n",
        "\n",
        "\n",
        "# Ensure y_scores_lstm are the probabilities for the positive class\n",
        "# Assuming the LSTM model outputs probabilities for each class in a one-hot encoded manner\n",
        "y_scores_lstm = lstm_model.predict(X_test_lstm)[:, 1] # Get probabilities for the positive class (assuming it's index 1)\n",
        "\n",
        "print(\"Shape of y_test_lstm_binary:\", y_test_lstm_binary.shape)\n",
        "print(\"Unique values in y_test_lstm_binary:\", np.unique(y_test_lstm_binary))\n",
        "print(\"Shape of y_scores_lstm:\", y_scores_lstm.shape)\n",
        "print(\"First 5 values of y_scores_lstm:\", y_scores_lstm[:5])\n",
        "\n",
        "plot_roc(y_test_lstm_binary, y_scores_lstm, \"LSTM\")"
      ],
      "metadata": {
        "id": "pfb32MsondQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def plot_roc_curve(model, X_test, y_test, model_name):\n",
        "    # âœ… Convert y_test to pandas Series for apply()\n",
        "    y_test_series = pd.Series(y_test)\n",
        "\n",
        "    # âœ… Convert multiclass labels to binary (functional = 1, non-functional = 0)\n",
        "    functional_types = ['FR', 'F']\n",
        "    y_test_binary = y_test_series.apply(lambda x: 1 if x in functional_types else 0)\n",
        "\n",
        "    # âœ… Get probability scores\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_scores = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_scores = model.decision_function(X_test)\n",
        "\n",
        "    # âœ… Sanity check\n",
        "    if len(y_test_binary) != len(y_scores):\n",
        "        raise ValueError(\"True labels and predicted scores must have the same number of samples.\")\n",
        "\n",
        "    # âœ… ROC curve calculation\n",
        "    fpr, tpr, _ = roc_curve(y_test_binary, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.plot(fpr, tpr, lw=2, label=f\"{model_name} (AUC={roc_auc:.2f})\")\n",
        "\n",
        "\n",
        "# âœ… Train models (with max_iter fix for LogisticRegression)\n",
        "lr_model = LogisticRegression(max_iter=500).fit(X_train, y_train)\n",
        "rf_model = RandomForestClassifier().fit(X_train, y_train)\n",
        "\n",
        "# âœ… Plot ROC curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "plot_roc_curve(lr_model, X_test, y_test, \"Logistic Regression\")\n",
        "plot_roc_curve(rf_model, X_test, y_test, \"Random Forest\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve Comparison\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JHHf9nWo06Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(y_test_bow))\n",
        "print(set(y_pred_lr))\n"
      ],
      "metadata": {
        "id": "y3Zbr3rxAQPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix\n",
        "def plot_confusion(y_true, y_pred, model_name):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-functional','Functional'], yticklabels=['Non-functional','Functional'])\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "gr6werNtnuBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df[\"label\"])   # \"Functional\", \"Non-Functional\", etc. ko numbers me convert\n",
        "\n",
        "# Split data\n",
        "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(\n",
        "    X_bow, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "lr.fit(X_train_bow, y_train_bow)\n",
        "\n",
        "# Predictions\n",
        "y_pred_lr = lr.predict(X_test_bow)\n",
        "\n",
        "#  Confusion Matrix\n",
        "cm = confusion_matrix(y_test_bow, y_pred_lr)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2p82hbMdA8Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion(y_test_lstm, y_pred_lstm, \"LSTM\")"
      ],
      "metadata": {
        "id": "8p2L_fdLn-0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction Pipeline Function\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
        "\n",
        "# Fit + Transform on training data\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "def predict_text(text, model, vectorizer, label_encoder):\n",
        "    seq = vectorizer.transform([text])\n",
        "    pred = model.predict(seq)\n",
        "    return label_encoder.inverse_transform(pred)[0]\n",
        "\n",
        "sample_text = \"This is a test requirement\"\n",
        "print(\"Prediction:\", predict_text(sample_text, log_reg, tfidf_vectorizer, le))\n"
      ],
      "metadata": {
        "id": "dKLVPDBO6__O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "df = pd.read_csv(\"software_requirements_extended.csv\")\n",
        "df[\"clean_text\"] = df[\"Requirement\"].astype(str)\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(df[\"clean_text\"])\n",
        "\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tfidf, f)\n",
        "\n",
        "print(\"âœ… TF-IDF Vectorizer saved\")\n"
      ],
      "metadata": {
        "id": "PZzdZpqqcWSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"Type\"])   # ya Requirement\n",
        "\n",
        "\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "print(\"âœ… Label Encoder saved\")\n"
      ],
      "metadata": {
        "id": "v-fMGDIzGXVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "with open(\"logistic_regression.pkl\", \"wb\") as f:\n",
        "    pickle.dump(log_reg, f)\n",
        "\n",
        "print(\"âœ… Logistic Regression model saved\")\n"
      ],
      "metadata": {
        "id": "DeI3RhU5GbUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "with open(\"naive_bayes.pkl\", \"wb\") as f:\n",
        "    pickle.dump(nb, f)\n",
        "\n",
        "print(\"âœ… Naive Bayes model saved\")\n"
      ],
      "metadata": {
        "id": "JB0jr0TbGhkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "with open(\"random_forest.pkl\", \"wb\") as f:\n",
        "    pickle.dump(rf, f)\n",
        "\n",
        "print(\"âœ… Random Forest model saved\")\n"
      ],
      "metadata": {
        "id": "gQGBBThtGlph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "with open(\"svm.pkl\", \"wb\") as f:\n",
        "    pickle.dump(svm, f)\n",
        "\n",
        "print(\"âœ… SVM model saved\")\n"
      ],
      "metadata": {
        "id": "rfqw-W2QGoaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "with open(\"xgb_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(xgb_model, f)\n",
        "\n",
        "print(\"âœ… XGBoost model saved\")\n"
      ],
      "metadata": {
        "id": "Kur6RksVGrET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_words = 2000\n",
        "max_len = 50\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df[\"clean_text\"])\n",
        "\n",
        "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"âœ… Tokenizer saved\")\n"
      ],
      "metadata": {
        "id": "k2iplzyEGtv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# LSTM Model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dense(32, activation=\"relu\"))\n",
        "lstm_model.add(Dense(1, activation=\"sigmoid\"))  # Binary classification\n",
        "\n",
        "lstm_model.compile(optimizer=Adam(0.001),\n",
        "                   loss=\"binary_crossentropy\",\n",
        "                   metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model (example)\n",
        "history = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# âœ… Save model in .keras format\n",
        "lstm_model.save(\"lstm_model.keras\")\n",
        "\n",
        "print(\"âœ… LSTM model saved as lstm_model.keras\")\n"
      ],
      "metadata": {
        "id": "OcQEvhHT33Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: BiLSTM model variable\n",
        "bilstm_model = Sequential()\n",
        "bilstm_model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
        "bilstm_model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
        "bilstm_model.add(Dropout(0.3))\n",
        "bilstm_model.add(Dense(64, activation=\"relu\"))\n",
        "bilstm_model.add(Dense(len(le.classes_), activation=\"softmax\"))\n",
        "\n",
        "bilstm_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "DP0ltn55dAHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained models\n",
        "cnn_model.save(\"cnn_model.keras\")\n",
        "print(\"âœ… CNN model saved\")\n",
        "\n",
        "bilstm_model.save(\"bilstm_model.keras\")\n",
        "print(\"âœ… BiLSTM model saved\")\n",
        "\n",
        "\n",
        "# Verify files exist\n",
        "import os\n",
        "print(\"Files in current directory:\", os.listdir())\n",
        "\n",
        "# Download to local machine\n",
        "from google.colab import files\n",
        "files.download(\"cnn_model.keras\")\n",
        "files.download(\"bilstm_model.keras\")\n"
      ],
      "metadata": {
        "id": "7Y4exKDkZkwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gen AI**"
      ],
      "metadata": {
        "id": "NEF4zzzYm2-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# STEP 0: Install dependencies\n",
        "# ----------------------------\n",
        "!pip install -q transformers pandas openpyxl reportlab tqdm coverage pytest sentencepiece matplotlib\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 1: Imports\n",
        "# ----------------------------\n",
        "import pandas as pd, os, re, tempfile, subprocess, traceback\n",
        "from tqdm import tqdm\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib.pagesizes import A4\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import pipeline, set_seed\n",
        "from google.colab import files\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 2: Upload dataset\n",
        "# ----------------------------\n",
        "print(\"âž¡ï¸ Upload your CSV file (software_requirements_extended.csv).\")\n",
        "uploaded = files.upload()\n",
        "CSV_FILE = list(uploaded.keys())[0]\n",
        "print(\"Uploaded:\", CSV_FILE)\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 3: Load dataset\n",
        "# ----------------------------\n",
        "df = pd.read_csv(CSV_FILE)\n",
        "if 'Requirement' in df.columns:\n",
        "    req_col = 'Requirement'\n",
        "elif 'requirement' in df.columns:\n",
        "    req_col = 'requirement'\n",
        "else:\n",
        "    text_cols = [c for c in df.columns if df[c].dtype == object]\n",
        "    req_col = text_cols[0]\n",
        "print(\"Using requirement column:\", req_col)\n",
        "df = df.dropna(subset=[req_col])\n",
        "print(\"Total requirements:\", len(df))\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 4: Load models\n",
        "# ----------------------------\n",
        "CODE_MODEL = \"Salesforce/codegen-350M-multi\"\n",
        "TEST_MODEL = \"google/flan-t5-small\"\n",
        "print(\"â³ Loading models (may take 30-90s)...\")\n",
        "code_generator = pipeline(\"text-generation\", model=CODE_MODEL, max_new_tokens=200, temperature=0.2)\n",
        "test_generator = pipeline(\"text2text-generation\", model=TEST_MODEL, max_length=128, do_sample=False)\n",
        "print(\"âœ… Models loaded.\")\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 5: Sanitizer helpers\n",
        "# ----------------------------\n",
        "import re\n",
        "def sanitize_code(s):\n",
        "    if s is None: return \"\"\n",
        "    s = re.sub(r\"```(?:python)?\", \"\", s)\n",
        "    s = re.sub(r\"https?://\\S+\", \"\", s)\n",
        "    s = re.sub(r\"(^\\s*#.*\\n)+\", \"\", s, flags=re.M)\n",
        "    s = re.split(r\"(?i)explanation:|answer:|output:\", s)[0]\n",
        "    m = re.search(r\"(def\\s+\\w+\\s*\\(.*\\):[\\s\\S]*$)\", s)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "    return s.strip()\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 6: Core generate+test\n",
        "# ----------------------------\n",
        "def generate_and_validate(requirement, timeout=25):\n",
        "    code_prompt = f\"Respond ONLY with valid Python code. Requirement:\\n{requirement}\"\n",
        "    code_raw = code_generator(code_prompt)[0]['generated_text']\n",
        "    code = sanitize_code(code_raw)\n",
        "    if len(code.strip()) == 0:\n",
        "        code = \"# ERROR: empty generation\"\n",
        "\n",
        "    test_prompt = f\"Write 1 pytest unit test function(s) for the following code. Only output the test functions:\\n{code}\"\n",
        "    test_raw = test_generator(test_prompt)[0]['generated_text']\n",
        "    tests = sanitize_code(test_raw)\n",
        "    if len(tests.strip()) == 0:\n",
        "        tests = \"\"\"\n",
        "def test_module_imports():\n",
        "    import generated_code\n",
        "    assert True\n",
        "\"\"\"\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        code_path = os.path.join(tmpdir, \"generated_code.py\")\n",
        "        test_path = os.path.join(tmpdir, \"test_generated_code.py\")\n",
        "        with open(code_path, \"w\", encoding=\"utf-8\") as f: f.write(code)\n",
        "        with open(test_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"import pytest\\nfrom generated_code import *\\n\\n\")\n",
        "            f.write(tests)\n",
        "\n",
        "        passed, failed, coverage_pct = 0, 0, \"0%\"\n",
        "        try:\n",
        "            subprocess.run([\"coverage\", \"erase\"], check=False, cwd=tmpdir)\n",
        "            proc = subprocess.run(\n",
        "                [\"coverage\", \"run\", \"-m\", \"pytest\", \"-q\", test_path],\n",
        "                capture_output=True, text=True, cwd=tmpdir, timeout=timeout\n",
        "            )\n",
        "            cov = subprocess.run([\"coverage\", \"report\", \"-m\"], capture_output=True, text=True, cwd=tmpdir)\n",
        "            out = proc.stdout + proc.stderr\n",
        "            m_pass = re.search(r\"(\\d+)\\s+passed\", out)\n",
        "            m_fail = re.search(r\"(\\d+)\\s+failed\", out)\n",
        "            passed = int(m_pass.group(1)) if m_pass else (1 if \"1 passed\" in out else 0)\n",
        "            failed = int(m_fail.group(1)) if m_fail else (1 if \"failed\" in out else 0)\n",
        "            match = re.search(r\"generated_code\\.py\\s+\\d+\\s+\\d+\\s+(\\d+%)\", cov.stdout)\n",
        "            if match:\n",
        "                coverage_pct = match.group(1)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            passed, failed, coverage_pct = 0, 1, \"0%\"\n",
        "        except Exception as e:\n",
        "            tb = traceback.format_exc()\n",
        "            code += \"\\n# RUNTIME EXCEPTION:\\n\" + tb\n",
        "            passed, failed, coverage_pct = 0, 1, \"0%\"\n",
        "\n",
        "    return {\n",
        "        \"requirement\": requirement,\n",
        "        \"generated_code\": code,\n",
        "        \"generated_tests\": tests,\n",
        "        \"pytest_stdout\": proc.stdout if 'proc' in locals() else \"\",\n",
        "        \"pytest_stderr\": proc.stderr if 'proc' in locals() else \"\",\n",
        "        \"coverage\": coverage_pct,\n",
        "        \"passed\": passed,\n",
        "        \"failed\": failed\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 7: Run on dataset (sample or full)\n",
        "# ----------------------------\n",
        "SAMPLE_N = 5\n",
        "sample_reqs = df[req_col].sample(SAMPLE_N, random_state=42).tolist()\n",
        "results = []\n",
        "for r in sample_reqs:\n",
        "    print(\"Processing:\", r[:80], \"...\")\n",
        "    res = generate_and_validate(r, timeout=25)\n",
        "    results.append(res)\n",
        "    print(\" -> done | passed:\", res['passed'], \"| coverage:\", res['coverage'])\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df['status'] = results_df['passed'].apply(lambda x: 'Pass' if x > 0 else 'Fail')\n",
        "results_df.to_csv(\"generated_code_results.csv\", index=False)\n",
        "results_df.to_excel(\"generated_code_results.xlsx\", index=False)\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 8: Graphs (Optimized)\n",
        "# ----------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Ensure numeric coverage\n",
        "coverage_values = results_df['coverage'].str.replace('%','').astype(int)\n",
        "\n",
        "# 1ï¸âƒ£ Pass/Fail Bar Chart\n",
        "plt.figure(figsize=(6,4))\n",
        "results_df['status'].value_counts().plot(kind='bar', color=['green','red'])\n",
        "plt.title(\"Pass vs Fail Counts\")\n",
        "plt.xlabel(\"Status\"); plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"pass_fail_bar.png\")\n",
        "plt.close()  # Close figure to avoid overlap\n",
        "\n",
        "# 2ï¸âƒ£ Coverage Histogram\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(coverage_values, bins=10, edgecolor='black', color='skyblue')\n",
        "plt.title(\"Coverage Distribution\")\n",
        "plt.xlabel(\"Coverage %\"); plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"coverage_hist.png\")\n",
        "plt.close()\n",
        "\n",
        "# 3ï¸âƒ£ Pie Chart (Pass/Fail Ratio)\n",
        "plt.figure(figsize=(5,5))\n",
        "results_df['status'].value_counts().plot(kind='pie', autopct='%1.1f%%', startangle=90, colors=['green','red'])\n",
        "plt.title(\"Pass/Fail Ratio\")\n",
        "plt.ylabel(\"\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"pass_fail_pie.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"âœ… All graphs saved as PNG.\")\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 9: PDF Report (Optimized)\n",
        "# ----------------------------\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image\n",
        "from reportlab.lib.styles import getSampleStyleSheet\n",
        "from reportlab.lib.pagesizes import A4\n",
        "\n",
        "pdf_path = \"proposal_and_results_summary.pdf\"\n",
        "styles = getSampleStyleSheet()\n",
        "doc = SimpleDocTemplate(pdf_path, pagesize=A4)\n",
        "elements = []\n",
        "\n",
        "elements.append(Paragraph(\"Generative AI â€” Code & Test Generation Report\", styles[\"Title\"]))\n",
        "elements.append(Spacer(1,12))\n",
        "elements.append(Paragraph(f\"Dataset: {CSV_FILE}\", styles[\"Normal\"]))\n",
        "elements.append(Paragraph(f\"Requirements processed: {len(results)}\", styles[\"Normal\"]))\n",
        "elements.append(Paragraph(f\"Pass Count: {results_df['passed'].sum()}\", styles[\"Normal\"]))\n",
        "elements.append(Paragraph(f\"Fail Count: {results_df['failed'].sum()}\", styles[\"Normal\"]))\n",
        "elements.append(Spacer(1,12))\n",
        "\n",
        "elements.append(Paragraph(\"Visual Insights\", styles[\"Heading2\"]))\n",
        "for img in [\"pass_fail_bar.png\", \"coverage_hist.png\", \"pass_fail_pie.png\"]:\n",
        "    if os.path.exists(img):\n",
        "        elements.append(Image(img, width=400, height=250))\n",
        "        elements.append(Spacer(1,12))\n",
        "\n",
        "elements.append(Paragraph(\"Per-requirement Summary:\", styles[\"Heading2\"]))\n",
        "for r in results:\n",
        "    brief = f\"- {r['requirement'][:80]}... | Passed: {r['passed']} | Coverage: {r['coverage']}\"\n",
        "    elements.append(Paragraph(brief, styles[\"Code\"]))\n",
        "elements.append(Spacer(1,12))\n",
        "\n",
        "doc.build(elements)\n",
        "print(\"âœ… PDF report generated:\", pdf_path)\n",
        "\n",
        "# ----------------------------\n",
        "# STEP 10: Download Files\n",
        "# ----------------------------\n",
        "for f in [\"generated_code_results.csv\",\"generated_code_results.xlsx\",pdf_path,\n",
        "          \"pass_fail_bar.png\",\"coverage_hist.png\",\"pass_fail_pie.png\"]:\n",
        "    if os.path.exists(f):\n",
        "        files.download(f)\n",
        "\n",
        "print(\"âœ… All files ready for download!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GUozI0gzIWcj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}